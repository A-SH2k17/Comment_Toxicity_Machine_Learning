{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a85c6f1-0af9-4e8f-a34d-c1677fa4c24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle \n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76d44fb5-d76d-4224-aa46-e600efab5468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Return the list of non-unique tokenized words. The function transforms \n",
    "    all text to lowercase, removes any punctuation and filters any non-ascii characters\n",
    "    if found. Lemmatize the word (Reducing the word to it's original root called lema\n",
    "    for example the lema of changed, changing , changes is change), and drop words \n",
    "    length < 3\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    #the following preprocessing code uses regular expression to match\n",
    "    #any punctuation any digits \n",
    "    #carriage returns, tabs ,and new lines\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
    "    nonpunctText = regex.sub(\" \",text)\n",
    "    words = nonpunctText.split(' ')\n",
    "    words = [word.encode('ascii','ignore').decode('ascii') for word in words]\n",
    "    lmtzer = WordNetLemmatizer()\n",
    "    words = [lmtzer.lemmatize(w) for w in words]\n",
    "    words = [w for w in words if len(w) >2]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac352c4c-bcd0-46f4-a842-e8186fab555a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ali_shar/opt/anaconda3/envs/AIprojectEnv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"Toxic-Comment-Classification-Challenge-master/data/train.csv\")\n",
    "vector = TfidfVectorizer(ngram_range=(1, 1), analyzer='word',\n",
    "                         tokenizer=tokenize, stop_words='english',\n",
    "                         strip_accents='unicode',use_idf=True, min_df=10)\n",
    "vector2 = vector.fit_transform(data[\"comment_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41e175ab-8d6e-4f2a-bf97-ecdd59036fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data.iloc[:,2:].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba347d2b-ac2b-4b99-858b-925681b8c35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = data.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62d64cee-87e4-457d-8ad1-c8d9e04914ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'comments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcomments\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'comments' is not defined"
     ]
    }
   ],
   "source": [
    "#comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f02d5afd-8655-462a-8e7e-0edf4dcb9750",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"project_variables/Xtrain.pickle\",'rb') as file:\n",
    "    comments = pickle.load(file)\n",
    "Xtrain,XTest,Ytrain,YTest = train_test_split(comments,y_labels,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "802e71a7-8f40-4cb5-aa52-f0611faa381e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = vector.transform([\"you are a dumb person\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae1b4750-4c01-4fe0-8280-77b4c730527c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ali_shar/opt/anaconda3/envs/AIprojectEnv/lib/python3.10/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.4.1.post1 when using version 1.4.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with open(\"project_variables/models/LogisticRegression.pickle\",'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59b5a278-e10b-4487-b9f9-693af1cbf52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic True\n",
      "severe_toxic False\n",
      "obscene False\n",
      "threat False\n",
      "insult True\n",
      "identity_hate False\n"
     ]
    }
   ],
   "source": [
    "for label in labels:\n",
    "    np.random.seed(42)\n",
    "    model.fit(Xtrain,Ytrain[label])\n",
    "    print(f\"{label} {'True' if model.predict(sample) else 'False'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db47d4c1-d0eb-4245-9a88-9b266e234550",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install --upgrade gradio jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5409b7b2-d6b2-4a87-a52b-170c0b0c4bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76179269-7823-4d8f-84be-14f9a88c4bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_comment(comment):\n",
    "    vectorized_comment = vector.transform([comment])\n",
    "    text = ''\n",
    "    for label in labels:\n",
    "        np.random.seed(42)\n",
    "        model.fit(Xtrain,Ytrain[label])\n",
    "        text+= '{}: {}   '.format(label,'True' if model.predict(vectorized_comment) else 'False')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03fde7f8-33a8-4f18-8196-89bfec91b3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "interface = gr.Interface(fn=score_comment,\n",
    "                        inputs=gr.components.Textbox(lines=2,placeholder='Comment to score'),\n",
    "                        outputs='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c988416a-3999-403b-8f3b-d7e297c9b063",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = score_comment(\"i hate you. you are the worst person and dumb\")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7515cf34-f88b-4049-af12-00bd9fcc9015",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'test\\ntest'.format()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33beb717-ac3d-49f0-9c55-17de48a51504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a8bd6c-2484-44ce-891e-58f93a22fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "interface.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ff8990-ac9f-493f-a36e-43373b5781fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
